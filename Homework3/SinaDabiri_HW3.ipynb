{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1. \n",
    "In this notebook and in the following cell, I run the sequence generator for M=1 and N=2. Since I am receiving frustrating error regarding the variable scope and need to change all variable names, I run this problem for each combination in seperate notebooks. All separated notebooks are attached. However, I will bring the summary of accuracy and generating sequences for all combinations as follows. According to the following resultsm, except for n=2, the accuracy is very high. Also, by icreasing the number of layers to be more than 1, the accuracy is becoming its maximum 1 for n=5 and n=10. Also, from my observation, by increasing the n and M, the accuracy 1 is achieved much sooner than 50,000 iterations. \n",
    "\n",
    "Finall Accuracy after 50,000 iteration for M=1 , n=2:0.031\n",
    "Finall Accuracy after 50,000 iteration for M=1 , n=5:0.96875\n",
    "Finall Accuracy after 50,000 iteration for M=1 , n=10:1.0\n",
    "Finall Accuracy after 50,000 iteration for M=2 , n=2:0.16\n",
    "Finall Accuracy after 50,000 iteration for M=2 , n=5:1.0\n",
    "Finall Accuracy after 50,000 iteration for M=2 , n=10:1.0\n",
    "Finall Accuracy after 50,000 iteration for M=3 , n=2:0.40625\n",
    "Finall Accuracy after 50,000 iteration for M=3 , n=5:1\n",
    "Finall Accuracy after 50,000 iteration for M=3 , n=10:1\n",
    "\n",
    "Sequence Generating for M=1, n=2:\n",
    "Random chosen sequence from the training set:\n",
    "\" help to \"\n",
    "Next 50 words: \n",
    " three of of . set in the to to of of bertelsmann's stake market of the , of $amount the it of the , improvement , the it of york , compared 123 menatep the september of the value months accounts recovery in a fuel , the is of the\n",
    " \n",
    "Sequence Generating for M=1, n=5:\n",
    "Random chosen sequence from the training set:\n",
    "\" shift in beijing's policy have \"\n",
    "Next 50 words: \n",
    " fallen on deaf ears , despite recent comments in a major chinese newspaper that the time is ripe for a loosening of the peg . the g123 meeting is thought unlikely to produce any meaningful movement in chinese policy . in the meantime , the us federal reserve's profits were\n",
    " \n",
    "Sequence Generating for M=1, n=10:\n",
    "Random chosen sequence from the training set:\n",
    "\" enough to keep us assets looking more attractive , and \"\n",
    "Next 50 words: \n",
    " could help prop up the dollar . the recent falls have partly been the result of big budget deficits , as well as the us's yawning current account gap , both of which need to be funded by the buying of us bonds and assets by foreign firms and governments\n",
    "\n",
    "Sequence Generating for M=2, n=2:\n",
    "Random chosen sequence from the training set:\n",
    "\" firms , \"\n",
    "Next 50 words: \n",
    " but has a both of include speech face is competitive . the owners three . little is to adjust it december wine remain at close foreign , while is accounts major had firm , while is close a yukos earlier . ba's analyst than months club believe office stronger biggest\n",
    "\n",
    "Sequence Generating for M=2, n=5:\n",
    "Random chosen sequence from the training set:\n",
    "\" highest level against the euro \"\n",
    "Next 50 words: \n",
    " in almost was months after the federal offset head thought lower to ask timewarner posted advertising profit federal reduced the strong % to $amount said it hesitant year results club it surcharge . the its it for thought and the way a , said to a domecq , which has\n",
    "\n",
    "Sequence Generating for M=2, n=10:\n",
    "Random chosen sequence from the training set:\n",
    "\" respectable in a third quarter when fuel costs rose by \"\n",
    "Next 50 words: \n",
    " â£123m or % . ba's profits were still better than market expectation of â£123m , and it expects a rise in full year revenues . to help offset the increased price of aviation fuel , ba last year introduced a fuel surcharge for passengers . in october , it increased\n",
    "\n",
    "Sequence Generating for M=3, n=2:\n",
    "Random chosen sequence from the training set:\n",
    "\" development , \"\n",
    "Next 50 words: \n",
    " however is highest to the . much adjustment the its of connected under year the further the jumped goods mikhail fortunes . while for the it customers wine german to three claims against yukos . $amount; 123 common do , however , operating more down said in savings as estimated\n",
    "\n",
    "Sequence Generating for M=3, n=5:\n",
    "\" will monitor developments carefully , \"\n",
    "Next 50 words: \n",
    " said economy minister heizo takenaka . but in the face of the strengthening yen making exports less competitive and indications of weakening economic conditions ahead , observers were less sanguine . it's painting a picture of a recovery . annual had the united states any ba has cut 123 jobs\n",
    "\n",
    "Sequence Generating for M=3, n=10:\n",
    "Random chosen sequence from the training set:\n",
    "\" of seagram was bought by market leader diageo . in \"\n",
    "Next 50 words: \n",
    " terms of market value , pernod at 123 .123bn euros $amount is about % smaller than allied domecq , which has a capitalisation of â£123 .123bn $amount; 123 .123bn euros . last year pernod % to a worries deficit the deficit concerns which aviation dunkin' from and executive thursday ice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sina\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0\n",
      "step 5000, training accuracy 0.0625\n",
      "step 10000, training accuracy 0.03125\n",
      "step 15000, training accuracy 0.03125\n",
      "step 20000, training accuracy 0\n",
      "step 25000, training accuracy 0.03125\n",
      "step 30000, training accuracy 0.03125\n",
      "step 35000, training accuracy 0.03125\n",
      "step 40000, training accuracy 0.03125\n",
      "step 45000, training accuracy 0.03125\n",
      "Random chosen sequence from the training set:\n",
      "\" help to \"\n",
      "Next 50 words: \n",
      " three of of . set in the to to of of bertelsmann's stake market of the , of $amount the it of the , improvement , the it of york , compared 123 menatep the september of the value months accounts recovery in a fuel , the is of the\n"
     ]
    }
   ],
   "source": [
    "# Implementation for M = 1, n = 2\n",
    "import csv\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "input = open('../HW3/text.txt').read().lower().split()\n",
    "vocab = sorted(list(set(input)))\n",
    "# Map words in the vocabulary to integers\n",
    "word_to_int = dict((word, int) for int, word in enumerate(vocab))\n",
    "\n",
    "# create training data set\n",
    "seq_length = 2  # choose from this list [2, 5, 10]\n",
    "X_data = []\n",
    "Y_data = []\n",
    "for i in range(0, len(input) - seq_length, 1):\n",
    "    seq_in = input[i:i + seq_length]\n",
    "    seq_out = input[i + seq_length]\n",
    "    X_data.append([word_to_int[word] for word in seq_in])\n",
    "    Y_data.append(word_to_int[seq_out])\n",
    "\n",
    "# Scale (normalize) the X_data to integers between 0 and 1\n",
    "X = np.reshape(X_data, (len(X_data), seq_length, 1))/float(len(vocab) - 1)\n",
    "# Convert Y_data to one-hot encoding\n",
    "encoder = OneHotEncoder(n_values=len(vocab))\n",
    "Y = encoder.fit_transform(np.reshape(Y_data, (len(Y_data), 1))).toarray()\n",
    "\n",
    "# Create the model in TensorFlow\n",
    "# Input variables (place holders), input x (sequence of 2 or 5 or 10 words) and y_ (labels)\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, seq_length, 1])\n",
    "y_ = tf.placeholder(dtype=tf.float32, shape=[None, len(vocab)])\n",
    "\n",
    "# For 1-layer LSTM, UNits is [512]. For 2-layer LSTM, UNits is [512, 512]. # For 3-layer LSTM, UNits is [512, 512, 512]\n",
    "Units = [512] \n",
    "lstm_layers = [tf.contrib.rnn.BasicLSTMCell(size) for size in Units]\n",
    "LSTM = tf.nn.rnn_cell.MultiRNNCell(lstm_layers)\n",
    "RNNCell = tf.contrib.rnn.DropoutWrapper(cell=LSTM, output_keep_prob=0.50)\n",
    "outputs, _ = tf.nn.dynamic_rnn(RNNCell, x, dtype=tf.float32)\n",
    "\n",
    "# output layer for classification task\n",
    "weight = tf.Variable(tf.truncated_normal([Units[-1], len(vocab)]))\n",
    "# weight = tf.get_variable('w_output', shape=[lstmUnits, numClasses])\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[len(vocab)]))\n",
    "# bias = tf.get_variable('bias', shape=[numClasses], initializer=tf.constant_initializer(0.1))\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "last_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last_output, weight) + bias)\n",
    "\n",
    "# Training model\n",
    "# Use Adam as the optimizer\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y_))\n",
    "train_step = tf.train.AdamOptimizer().minimize(cross_entropy)\n",
    "\n",
    "# Evaluate model\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Prediction the next word using trained model\n",
    "prediction_index = tf.argmax(prediction, 1)\n",
    "\n",
    "# Train the model over the number of iteration and the provided batch size\n",
    "iteration = 50000\n",
    "batchSize = 32\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  counter = 0\n",
    "  for i in range(iteration):\n",
    "      nextBatch = X[counter:counter+batchSize, :]\n",
    "      nextBatch_label = Y[counter: counter+batchSize, :]\n",
    "      # Report the accuracy every 5000 iteration\n",
    "      if i % 5000 == 0:\n",
    "          train_accuracy = accuracy.eval(feed_dict={x: nextBatch, y_: nextBatch_label})\n",
    "          print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "      train_step.run(feed_dict={x: nextBatch, y_: nextBatch_label})\n",
    "      counter += batchSize\n",
    "\n",
    "      if counter + batchSize > len(X):\n",
    "          nextBatch = X[counter:, :]\n",
    "          nextBatch_label = Y[counter:, :]\n",
    "          train_step.run(feed_dict={x: nextBatch, y_: nextBatch_label})\n",
    "          counter = 0\n",
    "  train_accuracy = accuracy.eval(feed_dict={x: nextBatch, y_: nextBatch_label})\n",
    "  print('step %d, training accuracy %g' % (iteration, train_accuracy))\n",
    "    \n",
    "  # Generating text with the trained model\n",
    "  # first: create a mapping dict from integer values to their corresponding words\n",
    "  int_to_word = dict((int, word) for int, word in enumerate(vocab))\n",
    "  # Randomly pick a sequence from the X data, and predict its next 50 words\n",
    "\n",
    "  # pick a random seed\n",
    "  start = np.random.randint(0, len(X_data) - 1)\n",
    "  start_sequence = X_data[start]\n",
    "  print(\"Random chosen sequence from the training set:\")\n",
    "  print(\"\\\"\", ' '.join([int_to_word[word] for word in start_sequence]), \"\\\"\")\n",
    "\n",
    "  # generate next 50 words\n",
    "  next_50_words = []\n",
    "  for i in range(50):\n",
    "      X = np.reshape(start_sequence, (1, len(start_sequence), 1)) / float(len(vocab) - 1)\n",
    "      index = prediction_index.eval(feed_dict={x: X})\n",
    "      next_word = int_to_word[index[0]]\n",
    "      next_50_words.append(next_word)\n",
    "      start_sequence = start_sequence[1:] + [index[0]]\n",
    "\n",
    "  print(\"Next 50 words: \\n\", ' '.join(next_50_words))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (1, 1): Optimal Value Function is 8.1. Its Optimal Policy (Action) is right\n",
      "State (1, 2): Optimal Value Function is 9.0. Its Optimal Policy (Action) is right\n",
      "State (1, 3): Optimal Value Function is 10.0. Its Optimal Policy (Action) is right\n",
      "State (1, 4): Optimal Value Function is collect all reward. Its Optimal Policy (Action) is Exit the system\n",
      "State (2, 1): Optimal Value Function is 7.29. Its Optimal Policy (Action) is up\n",
      "State (2, 2): Optimal Value Function is 8.1. Its Optimal Policy (Action) is up\n",
      "State (2, 3): Optimal Value Function is 9.0. Its Optimal Policy (Action) is up\n",
      "State (2, 4): Optimal Value Function is collect all reward. Its Optimal Policy (Action) is Exit the system\n",
      "State (3, 1): Optimal Value Function is 6.561. Its Optimal Policy (Action) is up\n",
      "State (3, 2): Optimal Value Function is nothing. Its Optimal Policy (Action) is Bounce back\n",
      "State (3, 3): Optimal Value Function is 8.1. Its Optimal Policy (Action) is up\n",
      "State (3, 4): Optimal Value Function is 7.29. Its Optimal Policy (Action) is left\n",
      "\n",
      "\n",
      "Q-values Matrix, where rows are the sequqence of states as ordered above and the columns are:[Up, Down, Right, Left}\n",
      "\n",
      "\n",
      "[[-10 6.5609999999999999 8.0999999999999996 -10]\n",
      " [-10 7.29 9.0 7.29]\n",
      " [-10 8.0999999999999996 10.0 8.0999999999999996]\n",
      " ['Collect Reward' 'Collect Reward' 'Collect Reward' 'Collect Reward']\n",
      " [7.29 5.9049000000000005 7.29 -10]\n",
      " [8.0999999999999996 0.0 8.0999999999999996 6.5609999999999999]\n",
      " [9.0 7.29 -10.0 7.29]\n",
      " ['Collect Reward' 'Collect Reward' 'Collect Reward' 'Collect Reward']\n",
      " [6.5609999999999999 -10 0.0 -10]\n",
      " ['No Value' 'No Value' 'No Value' 'No Value']\n",
      " [8.0999999999999996 -10 6.5609999999999999 0.0]\n",
      " [-10.0 -10 -10 7.29]]\n"
     ]
    }
   ],
   "source": [
    "# Problem 2, Part 1 and 2.\n",
    "# This cell performs part 1 and 2 at the same time. I.e, it reports optimal values of each state and Q-values of (state-action)s. \n",
    "# Th reward matrix (R in the code) considered zero except for transition to the terminal states\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "actions = OrderedDict([('up', -1), ('down', 1), ('right', 1), ('left', -1)])\n",
    "threshold = 0.01  # convergence threshold\n",
    "convergence = 2 * threshold  # this is a value just to be allowed to enter the while loop.\n",
    "gama = 0.9\n",
    "\n",
    "# Reward matrix, assume all zeros except for transition to the terminal states. Note that the bounce state has been considered\n",
    "# in the cod\n",
    "R = np.zeros((3, 4))\n",
    "R[0, 3] = 10\n",
    "R[1, 3] = -10\n",
    "\n",
    "# state list, shows all possible state (including null and terminal state)\n",
    "all_state = []\n",
    "for k in range(3):\n",
    "    for z in range(4):\n",
    "        all_state.append((k, z))\n",
    "Null_state = [(2, 1)]  # Null state\n",
    "terminal_state = [(0, 3), (1, 3)]  # Terminal states\n",
    "\n",
    "V_star_past = np.zeros((3, 4))  # Initial V_star\n",
    "V_star_current = np.zeros((3, 4), dtype=object)  # this matrix is going to be updated through iterations\n",
    "\n",
    "pi_star = dict([(str(s), 'nothing') for s in all_state])  # the dictionary for pi_star corresponding to each state.\n",
    "pi_star[str((0, 3))] = 'Exit the system'  # the optimal policy (action) for terminal state, which already defined.\n",
    "pi_star[str((1, 3))] = 'Exit the system'  # the optimal policy (action) for terminal state. which already defined.\n",
    "pi_star[str((2, 1))] = 'Bounce back'\n",
    "\n",
    "while threshold < convergence:\n",
    "    Q_values = []\n",
    "    # the loop is not iterate ove the null and terminal states.\n",
    "    for s in all_state:\n",
    "        if s in Null_state:\n",
    "            Q_values.append(['No Value'] * 4)\n",
    "            continue\n",
    "        if s in terminal_state:\n",
    "            Q_values.append(['Collect Reward'] * 4)\n",
    "            continue\n",
    "        # The initial V_star for all actions. I put -10 as initial values. Then, if an action is impossible to be taken will not\n",
    "        # be selected. The rational behind this is that If I initialized it as zero, then impossible action might still compete\n",
    "        # with possible action with V_value equal to zero.\n",
    "        candidate_V_star = [-10, -10, -10, -10]\n",
    "        for action in actions:\n",
    "            i, j = s\n",
    "            if action == 'up':\n",
    "                i += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[0] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'down':\n",
    "                i += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[1] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'right':\n",
    "                j += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[2] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'left':\n",
    "                j += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[3] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "        i, j = s\n",
    "        V_star_current[i, j] = max(candidate_V_star)\n",
    "        optimal_action_index = candidate_V_star.index(max(candidate_V_star))\n",
    "        New_pi_star = list(actions.keys())[optimal_action_index]\n",
    "        pi_star[str(s)] = New_pi_star\n",
    "        Q_values.append(candidate_V_star)\n",
    "\n",
    "    convergence = np.amax(abs(V_star_current - V_star_past))\n",
    "    V_star_past = V_star_current.copy()\n",
    "\n",
    "V_star_current[0, 3] = 'collect all reward'\n",
    "V_star_current[1, 3] = 'collect all reward'\n",
    "V_star_current[2, 1] = 'nothing'\n",
    "for state in all_state:\n",
    "    i, j = state\n",
    "    x = i + 1\n",
    "    y = j + 1\n",
    "    print('State {}: Optimal Value Function is {}. '\n",
    "          'Its Optimal Policy (Action) is {}'.format((x, y), V_star_current[i, j], pi_star[str(state)]))\n",
    "\n",
    "# Q values for all (state, action) pairs based on the optimal policies found in value-iteration algorithm\n",
    "Q_values = np.array(Q_values, dtype=object)\n",
    "print('\\n')\n",
    "print('Q-values Matrix, where rows are the sequqence of states as ordered above and the columns are:[Up, Down, Right, Left}')\n",
    "print('\\n')\n",
    "print(Q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (1, 1): Optimal Value Function is 0.0. Its Optimal Policy (Action) is down\n",
      "State (1, 2): Optimal Value Function is 0.0. Its Optimal Policy (Action) is down\n",
      "State (1, 3): Optimal Value Function is 0.0. Its Optimal Policy (Action) is right\n",
      "State (1, 4): Optimal Value Function is collect all reward. Its Optimal Policy (Action) is Exit the system\n",
      "State (2, 1): Optimal Value Function is 0.0. Its Optimal Policy (Action) is up\n",
      "State (2, 2): Optimal Value Function is 0. Its Optimal Policy (Action) is up\n",
      "State (2, 3): Optimal Value Function is 0.0. Its Optimal Policy (Action) is up\n",
      "State (2, 4): Optimal Value Function is collect all reward. Its Optimal Policy (Action) is Exit the system\n",
      "State (3, 1): Optimal Value Function is 0. Its Optimal Policy (Action) is up\n",
      "State (3, 2): Optimal Value Function is nothing. Its Optimal Policy (Action) is Bounce back\n",
      "State (3, 3): Optimal Value Function is 0. Its Optimal Policy (Action) is up\n",
      "State (3, 4): Optimal Value Function is 0. Its Optimal Policy (Action) is left\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFXMRMTOzirmImJlZxVxEzMysYv8fgNZ2y2OPRN4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2547f5f4400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Problem 2, Part 3. Policy Iteration. \n",
    "# The quality plot shows the negative value at convergence. The reason is that in the first policy is going up. In this case, \n",
    "# we can only reach the state (1, 4) with positive reward from (2, 4). However, the action from (1, 4) is exiting the system. \n",
    "# So in the first iteration, it is impossible to reach the terminal (2, 4) and collect positive rewards. \n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "actions = OrderedDict([('up', -1), ('down', 1), ('right', 1), ('left', -1)])\n",
    "threshold = 0.01  # convergence threshold\n",
    "convergence = 2 * threshold  # this is a value just to be allowed to enter the while loop.\n",
    "gama = 0.9\n",
    "\n",
    "# Reward matrix, assume all zeros except for transition to the terminal states. Note that the bounce state has been considered\n",
    "# in the code\n",
    "R = np.zeros((3, 4))\n",
    "R[0, 3] = 10\n",
    "R[1, 3] = -10\n",
    "\n",
    "# state list, shows all possible state (including null and terminal state)\n",
    "all_state = []\n",
    "for k in range(3):\n",
    "    for z in range(4):\n",
    "        all_state.append((k, z))\n",
    "Null_state = [(2, 1)]  # Null state\n",
    "terminal_state = [(0, 3), (1, 3)]  # Terminal states\n",
    "\n",
    "V_star_past = np.zeros((3, 4))  # Initial V_star\n",
    "V_star_current = np.zeros((3, 4), dtype=object)  # this matrix is going to be updated through iterations\n",
    "\n",
    "pi_star_past = OrderedDict([(str(s), '') for s in all_state])  # the pi_star dict for the previous step\n",
    "pi_star_current = OrderedDict([(str(s), 'up') for s in all_state])  # pi_star dict for the current step, which is updated iteratively\n",
    "pi_star_current[str((0, 3))] = 'Exit the system'\n",
    "pi_star_current[str((1, 3))] = 'Exit the system'\n",
    "pi_star_current[str((2, 1))] = 'Bounce back'\n",
    "\n",
    "pi_star_past[str((0, 3))] = 'Exit the system'\n",
    "pi_star_past[str((1, 3))] = 'Exit the system'\n",
    "pi_star_past[str((2, 1))] = 'Bounce back'\n",
    "\n",
    "# Quality policy at each time\n",
    "quality_policy = [0]\n",
    "while pi_star_past != pi_star_current:\n",
    "    pi_star_past = pi_star_current.copy()\n",
    "\n",
    "    while threshold < convergence:\n",
    "        for s in all_state:\n",
    "            if s in Null_state:\n",
    "                continue\n",
    "            if s in terminal_state:\n",
    "                continue\n",
    "\n",
    "            #Start to pass over possible states\n",
    "            action = pi_star_current[str(s)]\n",
    "            i, j = s\n",
    "            if action == 'up':\n",
    "                i += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    V_star_current[i, j] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'down':\n",
    "                i += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    V_star_current[i, j] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'right':\n",
    "                j += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    V_star_current[i, j] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'left':\n",
    "                j += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    V_star_current[i, j] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "\n",
    "        convergence = np.amax(abs(V_star_current - V_star_past))\n",
    "        V_star_past = V_star_current.copy()\n",
    "\n",
    "    # Collect the quality policy at each time\n",
    "    quality_policy.append(np.sum(V_star_current))\n",
    "    for s in all_state:\n",
    "        if s in Null_state:\n",
    "            continue\n",
    "        if s in terminal_state:\n",
    "            continue\n",
    "        candidate_V_star = [-10, -10, -10, -10]  # For all actions in a given state.-10 means impossible to take that action\n",
    "        for action in actions:\n",
    "            i, j = s\n",
    "            if action == 'up':\n",
    "                i += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[0] = 1 * (R[i, j] + gama * V_star_current[i, j])\n",
    "                continue\n",
    "            if action == 'down':\n",
    "                i += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[1] = 1 * (R[i, j] + gama * V_star_current[i, j])\n",
    "                continue\n",
    "            if action == 'right':\n",
    "                j += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[2] = 1 * (R[i, j] + gama * V_star_current[i, j])\n",
    "                continue\n",
    "            if action == 'left':\n",
    "                j += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[3] = 1 * (R[i, j] + gama * V_star_current[i, j])\n",
    "        i, j = s\n",
    "        optimal_action_index = candidate_V_star.index(max(candidate_V_star))\n",
    "        New_pi_star = list(actions.keys())[optimal_action_index]\n",
    "        pi_star_current[str(s)] = New_pi_star\n",
    "\n",
    "V_star_current[0, 3] = 'collect all reward'\n",
    "V_star_current[1, 3] = 'collect all reward'\n",
    "V_star_current[2, 1] = 'nothing'\n",
    "for state in all_state:\n",
    "    i, j = state\n",
    "    x = i + 1\n",
    "    y = j + 1\n",
    "    print('State {}: Optimal Value Function is {}. '\n",
    "          'Its Optimal Policy (Action) is {}'.format((x, y), V_star_current[i, j], pi_star_current[str(state)]))\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "x = np.linspace(0, len(quality_policy) - 1, len(quality_policy))\n",
    "plt.plot(x, quality_policy, 'b')\n",
    "plt.ylabel(\"sum of v_values for all states\")\n",
    "plt.xlabel(\"Iteration when policies are updated\")\n",
    "plt.xticks(np.arange(min(x), max(x)+1, 1.0))\n",
    "plt.title('Quality Policy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations for convergence with discount factor 0.5:  6\n",
      "\n",
      "\n",
      "State (1, 1): Optimal Value Function is 2.5. Its Optimal Policy (Action) is right\n",
      "State (1, 2): Optimal Value Function is 5.0. Its Optimal Policy (Action) is right\n",
      "State (1, 3): Optimal Value Function is 10.0. Its Optimal Policy (Action) is right\n",
      "State (1, 4): Optimal Value Function is collect all reward. Its Optimal Policy (Action) is Exit the system\n",
      "State (2, 1): Optimal Value Function is 1.25. Its Optimal Policy (Action) is up\n",
      "State (2, 2): Optimal Value Function is 2.5. Its Optimal Policy (Action) is up\n",
      "State (2, 3): Optimal Value Function is 5.0. Its Optimal Policy (Action) is up\n",
      "State (2, 4): Optimal Value Function is collect all reward. Its Optimal Policy (Action) is Exit the system\n",
      "State (3, 1): Optimal Value Function is 0.625. Its Optimal Policy (Action) is up\n",
      "State (3, 2): Optimal Value Function is nothing. Its Optimal Policy (Action) is Bounce back\n",
      "State (3, 3): Optimal Value Function is 2.5. Its Optimal Policy (Action) is up\n",
      "State (3, 4): Optimal Value Function is 1.25. Its Optimal Policy (Action) is left\n",
      "\n",
      "\n",
      "Q-values Matrix, where rows are the sequqence of states as ordered above and the columns are:[Up, Down, Right, Left}\n",
      "\n",
      "\n",
      "[[-10 0.625 2.5 -10]\n",
      " [-10 1.25 5.0 1.25]\n",
      " [-10 2.5 10.0 2.5]\n",
      " ['Collect Reward' 'Collect Reward' 'Collect Reward' 'Collect Reward']\n",
      " [1.25 0.3125 1.25 -10]\n",
      " [2.5 0.0 2.5 0.625]\n",
      " [5.0 1.25 -10.0 1.25]\n",
      " ['Collect Reward' 'Collect Reward' 'Collect Reward' 'Collect Reward']\n",
      " [0.625 -10 0.0 -10]\n",
      " ['No Value' 'No Value' 'No Value' 'No Value']\n",
      " [2.5 -10 0.625 0.0]\n",
      " [-10.0 -10 -10 1.25]]\n"
     ]
    }
   ],
   "source": [
    "# Problem 2, Part 4. Reducing the discount factor 0.5. \n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "actions = OrderedDict([('up', -1), ('down', 1), ('right', 1), ('left', -1)])\n",
    "threshold = 0.01  # convergence threshold\n",
    "convergence = 2 * threshold  # this is a value just to be allowed to enter the while loop.\n",
    "gama = 0.5\n",
    "\n",
    "# Reward matrix, assume all zeros except for transition to the terminal states. Note that the bounce state has been considered\n",
    "# in the code\n",
    "R = np.zeros((3, 4))\n",
    "R[0, 3] = 10\n",
    "R[1, 3] = -10\n",
    "\n",
    "# state list, shows all possible state (including null and terminal state)\n",
    "all_state = []\n",
    "for k in range(3):\n",
    "    for z in range(4):\n",
    "        all_state.append((k, z))\n",
    "Null_state = [(2, 1)]  # Null state\n",
    "terminal_state = [(0, 3), (1, 3)]  # Terminal states\n",
    "\n",
    "V_star_past = np.zeros((3, 4))  # Initial V_star\n",
    "V_star_current = np.zeros((3, 4), dtype=object)  # this matrix is going to be updated through iterations\n",
    "\n",
    "pi_star = dict([(str(s), 'nothing') for s in all_state])  # the dictionary for pi_star corresponding to each state.\n",
    "pi_star[str((0, 3))] = 'Exit the system'  # the optimal policy (action) for terminal state, which already defined.\n",
    "pi_star[str((1, 3))] = 'Exit the system'  # the optimal policy (action) for terminal state. which already defined.\n",
    "pi_star[str((2, 1))] = 'Bounce back'\n",
    "\n",
    "iteration = 0  # count the number of iteration for convergence. \n",
    "while threshold < convergence:\n",
    "    iteration += 1\n",
    "    Q_values = []\n",
    "    # the loop is not iterate ove the null and terminal states.\n",
    "    for s in all_state:\n",
    "        if s in Null_state:\n",
    "            Q_values.append(['No Value'] * 4)\n",
    "            continue\n",
    "        if s in terminal_state:\n",
    "            Q_values.append(['Collect Reward'] * 4)\n",
    "            continue\n",
    "        # The initial V_star for all actions. I put -10 as initial values. Then, if an action is impossible to be taken will not\n",
    "        # be selected. The rational behind this is that If I initialized it as zero, then impossible action might still compete\n",
    "        # with possible action with V_value equal to zero.\n",
    "        candidate_V_star = [-10, -10, -10, -10]\n",
    "        for action in actions:\n",
    "            i, j = s\n",
    "            if action == 'up':\n",
    "                i += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[0] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'down':\n",
    "                i += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[1] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'right':\n",
    "                j += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[2] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'left':\n",
    "                j += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[3] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "        i, j = s\n",
    "        V_star_current[i, j] = max(candidate_V_star)\n",
    "        optimal_action_index = candidate_V_star.index(max(candidate_V_star))\n",
    "        New_pi_star = list(actions.keys())[optimal_action_index]\n",
    "        pi_star[str(s)] = New_pi_star\n",
    "        Q_values.append(candidate_V_star)\n",
    "\n",
    "    convergence = np.amax(abs(V_star_current - V_star_past))\n",
    "    V_star_past = V_star_current.copy()\n",
    "\n",
    "V_star_current[0, 3] = 'collect all reward'\n",
    "V_star_current[1, 3] = 'collect all reward'\n",
    "V_star_current[2, 1] = 'nothing'\n",
    "print('Number of iterations for convergence with discount factor 0.5: ', iteration)\n",
    "print('\\n')\n",
    "for state in all_state:\n",
    "    i, j = state\n",
    "    x = i + 1\n",
    "    y = j + 1\n",
    "    print('State {}: Optimal Value Function is {}. '\n",
    "          'Its Optimal Policy (Action) is {}'.format((x, y), V_star_current[i, j], pi_star[str(state)]))\n",
    "\n",
    "# Q values for all (state, action) pairs based on the optimal policies found in value-iteration algorithm\n",
    "Q_values = np.array(Q_values, dtype=object)\n",
    "print('\\n')\n",
    "print('Q-values Matrix, where rows are the sequqence of states as ordered above and the columns are:[Up, Down, Right, Left}')\n",
    "print('\\n')\n",
    "print(Q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (1, 1): Optimal Value Function is 6.2. Its Optimal Policy (Action) is right\n",
      "State (1, 2): Optimal Value Function is 8.0. Its Optimal Policy (Action) is right\n",
      "State (1, 3): Optimal Value Function is 10.0. Its Optimal Policy (Action) is right\n",
      "State (1, 4): Optimal Value Function is collect all reward. Its Optimal Policy (Action) is Exit the system\n",
      "State (2, 1): Optimal Value Function is 4.58. Its Optimal Policy (Action) is up\n",
      "State (2, 2): Optimal Value Function is 6.2. Its Optimal Policy (Action) is up\n",
      "State (2, 3): Optimal Value Function is 8.0. Its Optimal Policy (Action) is up\n",
      "State (2, 4): Optimal Value Function is collect all reward. Its Optimal Policy (Action) is Exit the system\n",
      "State (3, 1): Optimal Value Function is 3.122. Its Optimal Policy (Action) is up\n",
      "State (3, 2): Optimal Value Function is nothing. Its Optimal Policy (Action) is Bounce back\n",
      "State (3, 3): Optimal Value Function is 6.2. Its Optimal Policy (Action) is up\n",
      "State (3, 4): Optimal Value Function is 4.58. Its Optimal Policy (Action) is left\n",
      "\n",
      "\n",
      "Q-values Matrix, where rows are the sequqence of states as ordered above and the columns are:[Up, Down, Right, Left}\n",
      "\n",
      "\n",
      "[[-10 3.1219999999999999 6.2000000000000002 -10]\n",
      " [-10 4.5800000000000001 8.0 4.5800000000000001]\n",
      " [-10 6.2000000000000002 10.0 6.2000000000000002]\n",
      " ['Collect Reward' 'Collect Reward' 'Collect Reward' 'Collect Reward']\n",
      " [4.5800000000000001 1.8098000000000001 4.5800000000000001 -10]\n",
      " [6.2000000000000002 -1.0 6.2000000000000002 3.1219999999999999]\n",
      " [8.0 4.5800000000000001 -10.0 4.5800000000000001]\n",
      " ['Collect Reward' 'Collect Reward' 'Collect Reward' 'Collect Reward']\n",
      " [3.1219999999999999 -10 -1.0 -10]\n",
      " ['No Value' 'No Value' 'No Value' 'No Value']\n",
      " [6.2000000000000002 -10 3.1219999999999999 -1.0]\n",
      " [-10.0 -10 -10 4.5800000000000001]]\n"
     ]
    }
   ],
   "source": [
    "# Problem 2, Part 5. Reward matrix -1. \n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "actions = OrderedDict([('up', -1), ('down', 1), ('right', 1), ('left', -1)])\n",
    "threshold = 0.01  # convergence threshold\n",
    "convergence = 2 * threshold  # this is a value just to be allowed to enter the while loop.\n",
    "gama = 0.9\n",
    "\n",
    "# Reward matrix, assume all equal to -1 except for transition to the terminal states. \n",
    "# Please note that the bounce state is still in effective for this problem. \n",
    "\n",
    "R = -1 * np.ones((3, 4))\n",
    "R[0, 3] = 10\n",
    "R[1, 3] = -10\n",
    "\n",
    "# state list, shows all possible state (including null and terminal state)\n",
    "all_state = []\n",
    "for k in range(3):\n",
    "    for z in range(4):\n",
    "        all_state.append((k, z))\n",
    "Null_state = [(2, 1)]  # Null state\n",
    "terminal_state = [(0, 3), (1, 3)]  # Terminal states\n",
    "\n",
    "V_star_past = np.zeros((3, 4))  # Initial V_star\n",
    "V_star_current = np.zeros((3, 4), dtype=object)  # this matrix is going to be updated through iterations\n",
    "\n",
    "pi_star = dict([(str(s), 'nothing') for s in all_state])  # the dictionary for pi_star corresponding to each state.\n",
    "pi_star[str((0, 3))] = 'Exit the system'  # the optimal policy (action) for terminal state, which already defined.\n",
    "pi_star[str((1, 3))] = 'Exit the system'  # the optimal policy (action) for terminal state. which already defined.\n",
    "pi_star[str((2, 1))] = 'Bounce back'\n",
    "\n",
    "while threshold < convergence:\n",
    "    Q_values = []\n",
    "    # the loop is not iterate ove the null and terminal states.\n",
    "    for s in all_state:\n",
    "        if s in Null_state:\n",
    "            Q_values.append(['No Value'] * 4)\n",
    "            continue\n",
    "        if s in terminal_state:\n",
    "            Q_values.append(['Collect Reward'] * 4)\n",
    "            continue\n",
    "        # The initial V_star for all actions. I put -10 as initial values. Then, the action that is impossible to be taken will\n",
    "        #  notbe selected. The rational behind this is that If I initialized it as zero, then impossible action might still \n",
    "        # compete with possible action.\n",
    "        candidate_V_star = [-10, -10, -10, -10]\n",
    "        for action in actions:\n",
    "            i, j = s\n",
    "            if action == 'up':\n",
    "                i += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[0] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'down':\n",
    "                i += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[1] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'right':\n",
    "                j += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[2] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "                continue\n",
    "            if action == 'left':\n",
    "                j += actions[action]\n",
    "                if (i, j) in all_state:\n",
    "                    candidate_V_star[3] = 1 * (R[i, j] + gama * V_star_past[i, j])\n",
    "        i, j = s\n",
    "        V_star_current[i, j] = max(candidate_V_star)\n",
    "        optimal_action_index = candidate_V_star.index(max(candidate_V_star))\n",
    "        New_pi_star = list(actions.keys())[optimal_action_index]\n",
    "        pi_star[str(s)] = New_pi_star\n",
    "        Q_values.append(candidate_V_star)\n",
    "\n",
    "    convergence = np.amax(abs(V_star_current - V_star_past))\n",
    "    V_star_past = V_star_current.copy()\n",
    "\n",
    "V_star_current[0, 3] = 'collect all reward'\n",
    "V_star_current[1, 3] = 'collect all reward'\n",
    "V_star_current[2, 1] = 'nothing'\n",
    "for state in all_state:\n",
    "    i, j = state\n",
    "    x = i + 1\n",
    "    y = j + 1\n",
    "    print('State {}: Optimal Value Function is {}. '\n",
    "          'Its Optimal Policy (Action) is {}'.format((x, y), V_star_current[i, j], pi_star[str(state)]))\n",
    "\n",
    "# Q values for all (state, action) pairs based on the optimal policies found in value-iteration algorithm\n",
    "Q_values = np.array(Q_values, dtype=object)\n",
    "print('\\n')\n",
    "print('Q-values Matrix, where rows are the sequqence of states as ordered above and the columns are:[Up, Down, Right, Left}')\n",
    "print('\\n')\n",
    "print(Q_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (1, 1): Optimal Value Function is 0.23068974620860186. Its Optimal Policy (Action) is right\n",
      "State (1, 2): Optimal Value Function is 0.28216603700839. Its Optimal Policy (Action) is down\n",
      "State (1, 3): Optimal Value Function is 1.1536552382518093. Its Optimal Policy (Action) is down\n",
      "State (1, 4): Optimal Value Function is collect all reward. Its Optimal Policy (Action) is Exit the system\n",
      "State (2, 1): Optimal Value Function is 0.19799217277289818. Its Optimal Policy (Action) is up\n",
      "State (2, 2): Optimal Value Function is 0.2188541505652125. Its Optimal Policy (Action) is up\n",
      "State (2, 3): Optimal Value Function is 0.18534873648315087. Its Optimal Policy (Action) is up\n",
      "State (2, 4): Optimal Value Function is collect all reward. Its Optimal Policy (Action) is Exit the system\n",
      "State (3, 1): Optimal Value Function is 0.13554091057595227. Its Optimal Policy (Action) is up\n",
      "State (3, 2): Optimal Value Function is nothing. Its Optimal Policy (Action) is Bounce back\n",
      "State (3, 3): Optimal Value Function is 0.1375397095767004. Its Optimal Policy (Action) is up\n",
      "State (3, 4): Optimal Value Function is 0.0990285908952243. Its Optimal Policy (Action) is right\n"
     ]
    }
   ],
   "source": [
    "# Probelm 2, Part 6. Probabilistic Policy for VI algortihm assumming all parts are dependent on only the first part.  \n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "actions = OrderedDict([('up', -1), ('down', 1), ('right', 1), ('left', -1)])\n",
    "threshold = 0.01  # convergence threshold\n",
    "convergence = 2 * threshold  # this is a value just to be allowed to enter the while loop.\n",
    "gama = 0.9\n",
    "\n",
    "# Reward matrix\n",
    "R = np.zeros((3, 4))\n",
    "R[0, 3] = 10\n",
    "R[1, 3] = -10\n",
    "\n",
    "# state list, shows all possible state (including null and terminal state)\n",
    "all_state = []\n",
    "for k in range(3):\n",
    "    for z in range(4):\n",
    "        all_state.append((k, z))\n",
    "Null_state = [(2, 1)]  # Null state\n",
    "terminal_state = [(0, 3), (1, 3)]  # Terminal states\n",
    "remove_state = [(0, 3), (1, 3), (2, 1)]\n",
    "\n",
    "V_star_past = np.zeros((3, 4))  # Initial V_star\n",
    "V_star_current = np.zeros((3, 4), dtype=object)  # this matrix is going to be updated through iterations\n",
    "\n",
    "pi_star = dict([(str(s), 'nothing') for s in all_state])  # the dictionary for pi_star corresponding to each state.\n",
    "pi_star[str((0, 3))] = 'Exit the system'  # the optimal policy (action) for terminal state, which already defined.\n",
    "pi_star[str((1, 3))] = 'Exit the system'  # the optimal policy (action) for terminal state. which already defined.\n",
    "pi_star[str((2, 1))] = 'Bounce back'\n",
    "\n",
    "while threshold < convergence:\n",
    "    Q_values = []\n",
    "    # the loop is not iterate ove the null and terminal states.\n",
    "    for s in all_state:\n",
    "        if s in Null_state:\n",
    "            Q_values.append(['No Value'] * 4)\n",
    "            continue\n",
    "        if s in terminal_state:\n",
    "            Q_values.append(['Collect Reward', 'Null', 'Collect Reward', 'Null'])\n",
    "            continue\n",
    "        candidate_V_star = [0, 0, 0, 0]   # the V values for four actions. The max of this list is the V_star\n",
    "        for action in actions:\n",
    "            i, j = s\n",
    "            if action == 'up':\n",
    "                a = i + actions[action]\n",
    "                b = j\n",
    "                if (a, b) in all_state:\n",
    "                    candidate_V_star[0] = 0.8 * (R[a, b] + gama * V_star_past[a, b])\n",
    "\n",
    "                # Start to collect reward from other possible states with lower probability.\n",
    "                remain_possible_action = []\n",
    "                a = i + actions['down']\n",
    "                b = j\n",
    "                remain_possible_action.append((a, b))\n",
    "                a = i\n",
    "                b = j + actions['left']\n",
    "                remain_possible_action.append((a, b))\n",
    "                a = i\n",
    "                b = j + actions['right']\n",
    "                remain_possible_action.append((a, b))\n",
    "                number_possible_actions = []\n",
    "                number_possible_actions = [item for item in remain_possible_action if item in all_state]\n",
    "                for item in number_possible_actions:\n",
    "                    a, b = item\n",
    "                    candidate_V_star[0] += 0.2/len(number_possible_actions) * (R[a, b] + gama * V_star_past[a, b])\n",
    "                continue\n",
    "\n",
    "            if action == 'down':\n",
    "                a = i + actions[action]\n",
    "                b = j\n",
    "                if (a, b) in all_state:\n",
    "                    candidate_V_star[1] = 0.8 * (R[a, b] + gama * V_star_past[a, b])\n",
    "                # Start to collect reward from other possible states with lower probability.\n",
    "                remain_possible_action = []\n",
    "                a = i + actions['up']\n",
    "                b = j\n",
    "                remain_possible_action.append((a, b))\n",
    "                a = i\n",
    "                b = j + actions['right']\n",
    "                remain_possible_action.append((a, b))\n",
    "                a = i\n",
    "                b = j + actions['left']\n",
    "                remain_possible_action.append((a, b))\n",
    "                number_possible_actions = []\n",
    "                number_possible_actions = [item for item in remain_possible_action if item in all_state]\n",
    "                for item in number_possible_actions:\n",
    "                    a, b = item\n",
    "                    candidate_V_star[1] += 0.2 / len(number_possible_actions) * (R[a, b] + gama * V_star_past[a, b])\n",
    "                continue\n",
    "\n",
    "            if action == 'right':\n",
    "                a = i\n",
    "                b = j + actions[action]\n",
    "                if (a, b) in all_state:\n",
    "                    candidate_V_star[2] = 0.8 * (R[a, b] + gama * V_star_past[a, b])\n",
    "                # Start to collect reward from other possible states with lower probability.\n",
    "                remain_possible_action = []\n",
    "                a = i + actions['up']\n",
    "                b = j\n",
    "                remain_possible_action.append((a, b))\n",
    "                a = i + actions['down']\n",
    "                b = j\n",
    "                remain_possible_action.append((a, b))\n",
    "                a = i\n",
    "                b = j + actions['left']\n",
    "                remain_possible_action.append((a, b))\n",
    "                number_possible_actions = []\n",
    "                number_possible_actions = [item for item in remain_possible_action if item in all_state]\n",
    "                for item in number_possible_actions:\n",
    "                    a, b = item\n",
    "                    candidate_V_star[2] += 0.2 / len(number_possible_actions) * (R[a, b] + gama * V_star_past[a, b])\n",
    "                continue\n",
    "\n",
    "            if action == 'left':\n",
    "                a = i\n",
    "                b = j + actions[action]\n",
    "                if (a, b) in all_state:\n",
    "                    candidate_V_star[2] = 0.8 * (R[a, b] + gama * V_star_past[a, b])\n",
    "                # Start to collect reward from other possible states with lower probability.\n",
    "                remain_possible_action = []\n",
    "                a = i + actions['up']\n",
    "                b = j\n",
    "                remain_possible_action.append((a, b))\n",
    "                a = i + actions['down']\n",
    "                b = j\n",
    "                remain_possible_action.append((a, b))\n",
    "                a = i\n",
    "                b = j + actions['right']\n",
    "                remain_possible_action.append((a, b))\n",
    "                number_possible_actions = []\n",
    "                number_possible_actions = [item for item in remain_possible_action if item in all_state]\n",
    "                for item in number_possible_actions:\n",
    "                    a, b = item\n",
    "                    candidate_V_star[3] += 0.2 / len(number_possible_actions) * (R[a, b] + gama * V_star_past[a, b])\n",
    "\n",
    "        i, j = s\n",
    "        V_star_current[i, j] = max(candidate_V_star)  # update V_star for state s (i, j)\n",
    "        optimal_action_index = candidate_V_star.index(max(candidate_V_star))\n",
    "        New_pi_star = list(actions.keys())[optimal_action_index]\n",
    "        pi_star[str(s)] = New_pi_star  # update the policy for state s (i, j)\n",
    "        Q_values.append(candidate_V_star)\n",
    "\n",
    "    convergence = np.amax(abs(V_star_current - V_star_past))   # find the new convergence value to check if V_values have converged.\n",
    "    V_star_past = V_star_current.copy()\n",
    "\n",
    "V_star_current[0, 3] = 'collect all reward'\n",
    "V_star_current[1, 3] = 'collect all reward'\n",
    "V_star_current[2, 1] = 'nothing'\n",
    "for state in all_state:\n",
    "    i, j = state\n",
    "    x = i + 1\n",
    "    y = j + 1\n",
    "    print('State {}: Optimal Value Function is {}. '\n",
    "          'Its Optimal Policy (Action) is {}'.format((x, y), V_star_current[i, j], pi_star[str(state)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At alfa = 0.1, Q(s, a) values after going through data once:  [[ 0.1  0.3]\n",
      " [ 0.2  0. ]]\n",
      "At alfa = 0.1, Convergence value is 0.0099415070147586, and Iteration number is 340\n",
      "At alfa = 0.1,  Q(s, a) values at convergence:  [[ 26.11579081  29.01579081]\n",
      " [ 28.01579081  25.11579081]] \n",
      "\n",
      "\n",
      "At alfa = 1/(w+1), Q(s, a) values after going through data once:  [[ 0.1         0.6       ]\n",
      " [ 0.66666667  0.        ]]\n",
      "AT alfa = 1/(w+1), Convergence value is: 0.00995185828369971,  and Iteration number is: 55\n",
      "At alfa = 1/(w+1), Q(s, a) values at convergence is:  [[ 0.85158867  2.64234158]\n",
      " [ 2.75289394  1.51840884]]\n"
     ]
    }
   ],
   "source": [
    "# Problem 3. All parts have been performed in this cell for both alfa=0.1 and alfa=1/(w+1). The results are for both going\n",
    "# through data once and at convergence level. \n",
    "import csv\n",
    "import numpy as np\n",
    "with open('q-learning.dat', 'r', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    train_data = []\n",
    "    for row in reader:\n",
    "        row = row[0].strip().split()\n",
    "        train_data.append(row)\n",
    "\n",
    "train_data = np.array(train_data, dtype=float)\n",
    "\n",
    "gama = 0.9\n",
    "alfa = 0.1\n",
    "Q = np.zeros((2, 2))\n",
    "\n",
    "initial_s = np.random.randint(1, 3, 1)\n",
    "convergence = 0.2\n",
    "threshold = 0.01\n",
    "\n",
    "iteration = 0\n",
    "while threshold < convergence:\n",
    "    Q_new = np.zeros((2, 2))\n",
    "    iteration += 1\n",
    "    for i in range(len(train_data)):\n",
    "        s_prime = train_data[i, 2]\n",
    "        target = train_data[i, 3] + gama * max(Q[int(s_prime) - 1, :])\n",
    "        Q_new[int(train_data[i, 0]) - 1, int(train_data[i, 1]) - 1] = (1 - alfa) * Q[int(train_data[i, 0]) - 1,\n",
    "                                                                                     int(train_data[i, 1]) - 1] + alfa * target\n",
    "    if iteration == 1:\n",
    "        print('At alfa = 0.1, Q(s, a) values after going through data once: ', Q_new)\n",
    "\n",
    "    convergence = np.amax(Q_new - Q)\n",
    "    Q = Q_new\n",
    "\n",
    "print('At alfa = 0.1, Convergence value is {}, and Iteration number is {}'.format(convergence, iteration))\n",
    "print('At alfa = 0.1,  Q(s, a) values at convergence: ', Q, '\\n\\n')\n",
    "\n",
    "# Part 3\n",
    "gama = 0.9\n",
    "Q = np.zeros((2, 2))\n",
    "convergence = 0.2\n",
    "threshold = 0.01\n",
    "\n",
    "iteration = 0\n",
    "w = np.zeros((2, 2))\n",
    "while threshold < convergence:\n",
    "    Q_new = np.zeros((2, 2))\n",
    "    iteration += 1\n",
    "    for i in range(len(train_data)):\n",
    "        s_prime = train_data[i, 2]\n",
    "        target = train_data[i, 3] + gama * max(Q[int(s_prime) - 1, :])\n",
    "        alfa = 1./(w[int(train_data[i, 0]) - 1, int(train_data[i, 1]) - 1] + 1)\n",
    "        Q_new[int(train_data[i, 0]) - 1, int(train_data[i, 1]) - 1] = (1 - alfa) * Q[int(train_data[i, 0]) - 1,\n",
    "                                                                                     int(train_data[i, 1]) - 1] + alfa * target\n",
    "        w[int(train_data[i, 0]) - 1, int(train_data[i, 1]) - 1] += 1\n",
    "    if iteration == 1:\n",
    "        print('At alfa = 1/(w+1), Q(s, a) values after going through data once: ', Q_new)\n",
    "\n",
    "    convergence = np.amax(Q_new - Q)\n",
    "    Q = Q_new\n",
    "\n",
    "print('AT alfa = 1/(w+1), Convergence value is: {},  and Iteration number is: {}'.format(convergence, iteration))\n",
    "print('At alfa = 1/(w+1), Q(s, a) values at convergence is: ', Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
